# -*- coding: utf-8 -*-
"""Copy of Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a643JKJ8jQB6rmZDJVLMHYI54CgaLS2E
"""

! pip install spacy==3.7.0
! python -m spacy download de_core_news_md

import numpy as np
import pandas as pd
import seaborn as sns

import os
import math
import time

import spacy
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer

import matplotlib.pyplot as plt
import plotly.figure_factory as ff
import plotly.graph_objects as go
import plotly.express as p

from wordcloud import WordCloud

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Read the CSV file
df = pd.read_csv("/content/JUH_Website_Scrape (1).csv", encoding='iso-8859-1')

df['Datum']= pd.to_datetime(df['Datum'])
df.drop(columns=['Bild URL'] , inplace=True)

# Title Length less than 4 will deleted
#df = df[df['Titel'].apply(lambda x: len(x.split())>4)]
df.shape

df.info()

# Drop duplicates based on the 'headline' column
df = df.drop_duplicates(subset='Titel', keep='first')
df.shape

df.sort_values('Titel',inplace=True, ascending=False)
duplicated_articles_series = df.duplicated('Titel', keep = False)
df = df[~duplicated_articles_series]
print("Total number of articles after removing duplicates:", df.shape[0])

df.isnull().sum()

df['Verband'].fillna('Alle Region', inplace=True)

#Remove null values
df = df.dropna()
print("Total number of articles : ", df.shape[0])
print("Total number of unqiue categories : ", df["Verband"].nunique())

df.info()

"""#Distribution of articles category-wise"""

fig = go.Figure([go.Bar(x=df["Verband"].value_counts().index, y=df["Verband"].value_counts().values)])
fig['layout'].update(title={"text" : 'Distribution of articles Verband','y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'}, xaxis_title="Verband name",yaxis_title="Number of articles")
fig.update_layout(width=800,height=700)
fig

"""## Month wise Articles"""

news_articles_per_month = df.resample('m',on = 'Datum')['Titel'].count()
news_articles_per_month

# Plotting
plt.figure(figsize=(12, 8))
ax = news_articles_per_month.plot(kind='bar', color='skyblue')

# Display only the month and year in the X-axis labels
ax.set_xticklabels(news_articles_per_month.index.strftime('%b %Y'), rotation=45, ha='right')

# Display the number on top of each bar
for i, v in enumerate(news_articles_per_month):
    ax.text(i, v + 0.1, str(v), ha='center', va='bottom')

plt.title('Number of Articles per Month')
plt.xlabel('Month and Year')
plt.ylabel('Number of Articles')
plt.show()

"""## Distribution of articles month-wis"""

fig = go.Figure([go.Bar(x=news_articles_per_month.index.strftime("%b"), y=news_articles_per_month)])
fig['layout'].update(title={"text" : 'Distribution of articles month-wise','y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'}, xaxis_title="Month",yaxis_title="Number of articles")
fig.update_layout(width=500,height=500)
fig

df.index = range(df.shape[0])

# Adding a new column containing both day of the week and month, it will be required later while recommending based on day of the week and month
df["Tag und Monat"] = df["Datum"].dt.strftime("%a") + "_" + df["Datum"].dt.strftime("%b")

"""#Text Preprocessing"""

def clean_text(text, keep_punctuation=False):
    """Cleans text by removing html tags, non ascii chars, digits and optionally punctuation"""

    import re

    # Compile RE pattern for HTTPS address, then Substitute it for blank
    RE_HTTPS = re.compile(r"https?://\S+ ")
    text = re.sub(RE_HTTPS, "", text)

    # Subsitute multiple points space for 1 point
    text = re.sub(r"\(?[.][.]+\)?", ".", text)

    # Compile RE pattern for HTML tags, then Substitute it for blank
    RE_TAGS = re.compile(r"<[^>]+>")
    text = re.sub(RE_TAGS, " ", text)

    # Compile RE patterns for general text, including punctuation rule
    if keep_punctuation:
        RE_ASCII = re.compile(r"[^a-züöä,.!?]", re.IGNORECASE)
        RE_SINGLECHAR = re.compile(r"\b[a-züöä,.!?]\b", re.IGNORECASE)
    else:
        RE_ASCII = re.compile(r"[^A-Za-zÀ-ž ]", re.IGNORECASE)
        RE_SINGLECHAR = re.compile(r"\b[A-Za-zÀ-ž]\b", re.IGNORECASE)

    # keep only ASCII + European Chars and whitespace, no digits
    text = re.sub(RE_ASCII, " ", text)
    # convert all whitespaces (tabs etc.) to single wspace
    text = re.sub(RE_SINGLECHAR, " ", text)

    # Subsitute multiple blank space for 1 blank space
    text = re.sub(r"\s+", " ", text)

    # Subsitute double punctuation (left-over after previous subsitutions) for 1 point
    text = re.sub(r" [.,]+ [,.]+", ".", text)

    return text

# Apply the preprocessing function to the specified columns
columns_to_preprocess = ['Titel', 'Teaser', 'Text']
for column in columns_to_preprocess:
    df[column] = df[column].apply(clean_text)

# Create a copy for text preprocessing
newdf = df.copy()
pd.set_option('display.max_colwidth', None)  # To display a very long headline completely

# Load Spacy trained pipelines for German
spacy_pipeline_de = spacy.load("de_core_news_md")
spacy_stopwords_de = set(spacy_pipeline_de.Defaults.stop_words)

# Load NLTK tokenizer and stemmer
nltk.download('punkt')
nltk.download('stopwords')
nltk_stemmer_de = SnowballStemmer("german")
nltk_stopwords_de = set(stopwords.words("german"))

# Define normalization function
def normalize(text, stopwords, stemmer=None, lemmatizer=None):
    """Normalizes a text using tokenization, stemming or lemmatization, and removes stopwords"""
    # Apply Word Tokenization
    word_tokens = word_tokenize(text)

    if lemmatizer is not None:
        doc = lemmatizer(text, disable=['tagger', 'parser', 'ner'])
        return ' '.join([tok.lemma_.lower() for tok in doc if tok.is_alpha and not tok.is_punct and tok.text.lower() not in stopwords and tok.lemma_.lower() not in stopwords])
    elif stemmer is not None:
        return ' '.join([stemmer.stem(word.lower()) for word in word_tokens if word.lower() not in stopwords])
    else:
        return ' '.join([word.lower() for word in word_tokens if word.lower() not in stopwords])

# Define a function to create new normalized columns
def create_normalized_columns(df, text_columns, stopwords, stemmer=None, lemmatizer=None):
    for column in text_columns:
        new_column_name = f"{column}_normalized"  # Create a new column name
        df[new_column_name] = df[column].apply(
            lambda x: normalize(x, stopwords, stemmer=stemmer, lemmatizer=lemmatizer))
    return df

# Assuming your DataFrame is named df and you have text columns 'Titel', 'Teaser', and 'Text'
text_columns_to_normalize = ['Titel', 'Teaser', 'Text']
df_with_normalized_columns = create_normalized_columns(df, text_columns_to_normalize, nltk_stopwords_de, lemmatizer=spacy_pipeline_de)

# Below libraries are for feature representation using sklearn
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

# Below libraries are for similarity matrices using sklearn
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import pairwise_distances

newdf['combined_text'] = newdf['Titel_normalized'] + ' ' + newdf['Teaser_normalized'] + ' ' + newdf['Text_normalized']

newdf.info()

newdf.columns

"""##Using TF-IDF method

"""

tfidf_headline_vectorizer = TfidfVectorizer(min_df = 0)
tfidf_headline_features = tfidf_headline_vectorizer.fit_transform(newdf['combined_text'])

def tfidf_based_model(row_index, num_similar_items):
    couple_dist = pairwise_distances(tfidf_headline_features,tfidf_headline_features[row_index])
    indices = np.argsort(couple_dist.ravel())[0:num_similar_items]
    df = pd.DataFrame({'publish_date': newdf['Datum'][indices].values,
               'headline':newdf['Titel'][indices].values,
                'Euclidean similarity with the queried article': couple_dist[indices].ravel()})
    print("="*30,"Queried article details","="*30)
    print('headline : ',newdf['Titel'][indices[0]])
    print("\n","="*25,"Recommended articles : ","="*23)

    return df.iloc[1:,]


tfidf_based_model(133, 4)

"""##Using Word2Vec embedding"""

!pip install wget
!pip install gensim

import wget
model_url = 'https://cloud.devmount.de/d2bc5672c523b086/german.model'
# Download the model files
model_filename = wget.download(model_url)

from gensim.models import Word2Vec
from gensim.models import KeyedVectors

# Assuming you've downloaded the embeddings to Colab's "/content" directory
model = KeyedVectors.load_word2vec_format("/content/german.model", binary=True)  # Adjust filename

vocabulary = model.index_to_key  # Access vocabulary
w2v_headline = []

for headline in newdf['Titel']:
    w2Vec_word = np.zeros(model.vector_size, dtype="float32")  # Use model's vector size
    for word in headline.split():
        if word in vocabulary:
            w2Vec_word = np.add(w2Vec_word, model[word])  # Access embeddings directly
    w2Vec_word = np.divide(w2Vec_word, len(headline.split()))
    w2v_headline.append(w2Vec_word)

w2v_headline = np.array(w2v_headline)

newdf.columns

def avg_w2v_based_model(row_index, num_similar_items):
    couple_dist = pairwise_distances(w2v_headline, w2v_headline[row_index].reshape(1,-1))
    indices = np.argsort(couple_dist.ravel())[0:num_similar_items]
    df = pd.DataFrame({'publish_date': newdf['Datum'][indices].values,
               'headline':newdf['Titel'][indices].values,
                'Euclidean similarity with the queried article': couple_dist[indices].ravel()})
    print("="*30,"Queried article details","="*30)
    print('headline : ',newdf['Titel'][indices[0]])
    print("\n","="*25,"Recommended articles : ","="*23)
    #return df.iloc[1:,1]
    return df.iloc[1:,]

avg_w2v_based_model(133, 11)

# Define function to get text representation using Word2Vec
def get_text_embedding(text, model):
    words = text.split()
    word_vectors = [model[word] for word in words if word in model]
    if word_vectors:
        return np.mean(word_vectors, axis=0)  # Average word vectors
    else:
        return np.zeros(model.vector_size)  # Handle empty text or OOV words

# Function for recommending similar items using Word2Vec and cosine similarity
def recommend_similar_items(query_text, df, model, num_results=5):
    query_embedding = get_text_embedding(query_text, model)
    all_embeddings = np.vstack([get_text_embedding(text, model) for text in newdf['combined_text']])
    cosine_similarities = 1 - pairwise_distances(all_embeddings, query_embedding.reshape(1, -1), metric='cosine')
    indices = np.argsort(cosine_similarities)[0][-num_results:][::-1]  # Get top results in descending order
    return df.iloc[indices]  # Return recommended items

# Example usage
#query_text = "bung macht den Meister"  # Replace with your query text
#recommended_items = recommend_similar_items(query_text, newdf, model)
#recommended_items
# Display only specified columns
#columns_to_display = ["Datum", "Titel", "Verband", "Titel_URL"]
#recommended_items[columns_to_display]

"""##Weighted similarity based on headline and category"""

from sklearn.preprocessing import OneHotEncoder

category_onehot_encoded = OneHotEncoder().fit_transform(np.array(newdf["Verband"]).reshape(-1,1))

def avg_w2v_with_category(row_index, num_similar_items, w1,w2): #headline_preference = True, category_preference = False):
    w2v_dist  = pairwise_distances(w2v_headline, w2v_headline[row_index].reshape(1,-1))
    category_dist = pairwise_distances(category_onehot_encoded, category_onehot_encoded[row_index]) + 1
    weighted_couple_dist   = (w1 * w2v_dist +  w2 * category_dist)/float(w1 + w2)
    indices = np.argsort(weighted_couple_dist.flatten())[0:num_similar_items].tolist()
    df = pd.DataFrame({'publish_date': newdf['Datum'][indices].values,
               'headline':newdf['Titel'][indices].values,
                'Weighted Euclidean similarity with the queried article': weighted_couple_dist[indices].ravel(),
                'Word2Vec based Euclidean similarity': w2v_dist[indices].ravel(),
                 'Category based Euclidean similarity': category_dist[indices].ravel(),
                'Categoty': newdf['Verband'][indices].values})

    print("="*30,"Queried article details","="*30)
    print('headline : ',newdf['Titel'][indices[0]])
    print('Categoty : ', newdf['Verband'][indices[0]])
    print("\n","="*25,"Recommended articles : ","="*23)
    #return df.iloc[1:,[1,5]]
    return df.iloc[1:, ]

avg_w2v_with_category(528,10,0.1,0.8)

"""##Weighted similarity based on headline, category, author and publishing day"""

publishingday_onehot_encoded = OneHotEncoder().fit_transform(np.array(newdf["Tag und Monat"]).reshape(-1,1))

newdf.columns

def avg_w2v_with_category_authors_and_publshing_day(row_index, num_similar_items, w1,w2,w3,w4): #headline_preference = True, category_preference = False):
    w2v_dist  = pairwise_distances(w2v_headline, w2v_headline[row_index].reshape(1,-1))
    category_dist = pairwise_distances(category_onehot_encoded, category_onehot_encoded[row_index]) + 1
    publishingday_dist = pairwise_distances(publishingday_onehot_encoded, publishingday_onehot_encoded[row_index]) + 1
    weighted_couple_dist   = (w1 * w2v_dist +  w2 * category_dist  + w3 * publishingday_dist)/float(w1 + w2 + w3)
    indices = np.argsort(weighted_couple_dist.flatten())[0:num_similar_items].tolist()
    df = pd.DataFrame({'publish_date': newdf['Datum'][indices].values,
                'headline_text':newdf['Titel'][indices].values,
                'Weighted Euclidean similarity with the queried article': weighted_couple_dist[indices].ravel(),
                'Word2Vec based Euclidean similarity': w2v_dist[indices].ravel(),
                'Category based Euclidean similarity': category_dist[indices].ravel(),
                'Publishing day based Euclidean similarity': publishingday_dist[indices].ravel(),
                'Categoty': newdf['Verband'][indices].values,
                'Day and month': newdf['Tag und Monat'][indices].values})
    print("="*30,"Queried article details","="*30)
    print('headline : ',newdf['Titel'][indices[0]])
    print('Categoty : ', newdf['Verband'][indices[0]])
    print('Day and month : ', newdf['Tag und Monat'][indices[0]])
    print("\n","="*25,"Recommended articles : ","="*23)
    #return df.iloc[1:,[1,7,8,9]]
    return df.iloc[1:, ]


avg_w2v_with_category_authors_and_publshing_day(528,10,0.6,0.2,0.1,0.2)

